name: Unit Tests Only

on:
  push:
    branches:
      - main
      - master
      - develop
    paths:
      - 'lambda_functions/**'
      - 'tests/**'
      - 'app.py'
      - 'requirements.txt'
      - 'pytest.ini'
  pull_request:
    branches:
      - main
      - master
    paths:
      - 'lambda_functions/**'
      - 'tests/**'
      - 'app.py'
      - 'requirements.txt'
      - 'pytest.ini'
  workflow_dispatch:  # Allow manual triggering

env:
  PYTHON_VERSION: '3.9'

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    
    permissions:
      contents: read
      pull-requests: write  # For PR comments
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run Unit Tests with Coverage
      id: tests
      continue-on-error: true  # Don't fail the workflow if tests fail
      run: |
        echo "üß™ Running Serverless Snacks Unit Tests"
        echo "======================================="
        
        # Create test reports directory
        mkdir -p test-reports
        
        # Run tests with multiple output formats
        pytest tests/ \
          --junitxml=test-reports/junit.xml \
          --cov=lambda_functions \
          --cov=app \
          --cov-report=xml:test-reports/coverage.xml \
          --cov-report=html:test-reports/htmlcov \
          --cov-report=term-missing \
          --cov-branch \
          --cov-fail-under=0 \
          -v \
          -x \
          --tb=short
        
        # Capture exit code for later use
        echo "TEST_EXIT_CODE=$?" >> $GITHUB_ENV
    
    - name: Generate Test Summary
      if: always()
      run: |
        echo "üìä Test Execution Summary" >> $GITHUB_STEP_SUMMARY
        echo "========================" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f test-reports/junit.xml ]; then
          # Parse JUnit XML for basic stats (simplified)
          echo "‚úÖ **Test Results Generated**" >> $GITHUB_STEP_SUMMARY
          echo "- JUnit XML: Available in artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- Coverage HTML: Available in artifacts" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ "${TEST_EXIT_CODE}" = "0" ]; then
          echo "üéâ **All tests passed!**" >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ö†Ô∏è **Some tests failed or had issues**" >> $GITHUB_STEP_SUMMARY
          echo "Check the test logs and artifacts for details." >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Note**: Test failures do not block deployments." >> $GITHUB_STEP_SUMMARY
    
    - name: Upload Test Results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: unit-test-results-${{ github.run_number }}
        path: |
          test-reports/
        retention-days: 30
    
    - name: Upload Coverage to Codecov (Optional)
      if: always() && github.event_name == 'push'
      continue-on-error: true
      uses: codecov/codecov-action@v3
      with:
        file: test-reports/coverage.xml
        flags: unittests
        name: codecov-umbrella
    
    - name: Test Status Check
      if: always()
      run: |
        if [ "${TEST_EXIT_CODE}" = "0" ]; then
          echo "‚úÖ Unit tests completed successfully"
        else
          echo "‚ö†Ô∏è Unit tests completed with issues (not blocking deployment)"
        fi